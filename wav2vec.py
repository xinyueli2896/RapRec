# -*- coding: utf-8 -*-
"""wav2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pLtBgyHj3SHXBIiae4POR_YlsrItjLih
"""

!pip install transformers
!pip install datasets
!pip install pydub

!pip install demucs==2.0.0

import soundfile as sf
import torch
import librosa
import torch
import torch.nn as nn
import torchaudio
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from datasets import load_dataset
# from demucs import pretrained
from demucs.pretrained import load_pretrained
# from demucs.pretrained import demucs_unittest
from transformers import AutoTokenizer, Wav2Vec2ForCTC, Wav2Vec2Processor

input_audio_file = '/content/Blessings.flac'
lrc_file = '/content/Blessings - The Kid LAROI.lrc'#lyrics start at line 4(is it always...?)

# chunkize
from pydub import AudioSegment
import re
import os

def parse_lrc(lrc_content):
    """
    Parse the LRC file content and extract timestamps and lyrics.
    Returns a list of tuples (start_time, end_time, lyric).
    """
    timestamps = []
    lyrics = []
    for line in lrc_content.split('\n'):
        match = re.match(r'\[(\d+):(\d+\.\d+)\](.*)', line)
        if match:
            minutes, seconds, lyric = match.groups()
            time = int(minutes) * 60 * 1000 + int(float(seconds) * 1000)  # Convert to milliseconds
            timestamps.append(time)
            lyrics.append(lyric)

    # Create tuples of (start_time, end_time, lyric)
    intervals = [(timestamps[i], timestamps[i+1], lyrics[i]) for i in range(len(timestamps)-1)]
    return intervals

def split_audio_and_lyrics(audio_file, lrc_content, output_folder):
    """
    Split the audio file into chunks and create text files for each chunk with the corresponding lyrics.
    """
    audio = AudioSegment.from_file(audio_file, format="flac")
    intervals = parse_lrc(lrc_content)

    for i, (start, end, lyric) in enumerate(intervals):
        chunk = audio[start:end]
        chunk.export(f"{output_folder}/chunk_{i+1}.mp3", format="mp3")

        with open(f"{output_folder}/chunk_{i+1}.txt", 'w') as text_file:
            text_file.write(lyric)

output_folder = '/content/trial1'

if not os.path.exists(output_folder):
    os.makedirs(output_folder)

with open(lrc_file, 'r') as file:
    lrc_content = file.read()

split_audio_and_lyrics(input_audio_file, lrc_content, output_folder)

class ChangeSampleRate(nn.Module):
    def __init__(self, input_rate: int, output_rate: int):
        super().__init__()
        self.output_rate = output_rate
        self.input_rate = input_rate

    def forward(self, wav: torch.tensor) -> torch.tensor:
        # Only accepts 1-channel waveform input
        wav = wav.view(wav.size(0), -1)
        new_length = wav.size(-1) * self.output_rate // self.input_rate
        indices = (torch.arange(new_length) * (self.input_rate / self.output_rate))
        round_down = wav[:, indices.long()]
        round_up = wav[:, (indices.long() + 1).clamp(max=wav.size(-1) - 1)]
        output = round_down * (1. - indices.fmod(1.)).unsqueeze(0) + round_up * indices.fmod(1.).unsqueeze(0)
        return output

class MyWav2Vec(nn.Module):
    def __init__(self):
      super().__init__()
      self.demucs = load_pretrained('demucs')
      self.wav2vec = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
      self.wav2vec_processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
      self.changesamplerate = torchaudio.transforms.Resample(44100, 16000)

    def forward(self, input_tensor):

      # remove the commented part to see what its' like with the source separation
      # #Demucs:
      # out_demucs = self.demucs(input_tensor)
      # print(out_demucs.shape)
      # #Extract voice and squeeze:
      # output_voice = out_demucs[:, 3, :, :]
      # print(output_voice.shape)
      # #transform from stereo to mono:
      output_voice_mono = torch.mean(input_tensor, dim=1)

      # change sample rate:
      audio_sr = self.changesamplerate(output_voice_mono)

      #Wav2Vec processor function:
      input_values = (audio_sr - audio_sr.mean(1)) / torch.sqrt(audio_sr.var(1) + 1e-5)
      # input_values = torch.reshape(input_values, (audio_sr.shape[1],audio_sr.shape[2]))
      # print(input_values.shape)
      #Wav2Vec:
      logits = self.wav2vec(input_values).logits
      predicted_ids = torch.argmax(logits, dim=-1)
      transcription = self.wav2vec_processor.decode(predicted_ids[0])

      return transcription

audio, _ = torchaudio.load("/content/trial1/chunk_11.mp3")
print(type(audio))
sr = 44100
input_tensor_model = torch.cat([audio.unsqueeze(0)])
# print(input_tensor_model.shape)
Mymodel = MyWav2Vec()
transcription = Mymodel(input_tensor_model)
print(transcription)