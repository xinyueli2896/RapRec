{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "QJjamukLqO12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "tvNe9yWvqSjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import librosa"
      ],
      "metadata": {
        "id": "E5cGHkTWqke3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9tZaMrHwHCo",
        "outputId": "6514aa05-8167-4024-92ec-401970ad2ee5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/data/songs/Ballin.flac\" #customize it\n",
        "audio_input, sample_rate = librosa.load(file_path, sr=16000)"
      ],
      "metadata": {
        "id": "JWHRpZeIrHDX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# librispeech_samples_ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "\n",
        "# # load audio\n",
        "# audio_input, sample_rate = sf.read(librispeech_samples_ds[0][\"file\"])\n",
        "# inputs = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values"
      ],
      "metadata": {
        "id": "iKToXo6l2fWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values"
      ],
      "metadata": {
        "id": "hch24fAyvDfv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZqs22_L2UYW",
        "outputId": "7d3ac777-5ab2-4e9c-dc56-693b42b158dc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[7.5996e-05, 7.5846e-05, 7.5893e-05,  ..., 8.9248e-05, 9.2605e-05,\n",
              "         7.6219e-05]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(inputs).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# transcribe\n",
        "transcription = processor.decode(predicted_ids[0])"
      ],
      "metadata": {
        "id": "SoZRquYA2T2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}