{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh1XAy2mPkJlNklC/o739s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinyueli2896/raptranscription/blob/main/loader_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "QyqlqkPAWbz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a706eb-92ce-4be1-d6ed-b02c429cee21"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install eyed3"
      ],
      "metadata": {
        "id": "TYMrppy6uG25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import eyed3\n",
        "import csv\n",
        "import re\n",
        "import json"
      ],
      "metadata": {
        "id": "HNNus7xYi4s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RplLzUG8VR-p"
      },
      "outputs": [],
      "source": [
        "# @title clean\n",
        "def remove_duplicates_and_corrupts(dir):\n",
        "    for filename in os.listdir(dir):\n",
        "        file_path = os.path.join(dir, filename)\n",
        "        try: #remove corrupted file\n",
        "            if eyed3.load(file_path) is None:\n",
        "                # File can't be loaded, delete it\n",
        "                os.remove(file_path)\n",
        "                print(f\"Deleted corrupt file: {filename}\")\n",
        "        except Exception as e:\n",
        "            # Handle any other exception\n",
        "            print(f\"Error with file {filename}: {e}\")\n",
        "\n",
        "        name = filename.split('.')[0]\n",
        "        if name.endswith(')'): #remove duplicates\n",
        "           os.remove(os.path.join(dir,filename))\n",
        "           print(f\"Deleted duplicated file: {filename}\")\n",
        "def append_rows_to_csv(file_path, new_rows):\n",
        "    with open(file_path, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Check if new_rows is a list of lists\n",
        "        if not all(isinstance(row, list) for row in new_rows):\n",
        "            # If new_rows is just a single list, wrap it in another list\n",
        "            new_rows = [new_rows]\n",
        "        writer.writerows(new_rows)\n",
        "        print(new_rows)\n",
        "\n",
        "\n",
        "remove_duplicates_and_corrupts(\"/content/drive/MyDrive/data/dataset_cleaned/train\")\n",
        "remove_duplicates_and_corrupts(\"/content/drive/MyDrive/data/dataset2_cleaned/test\")\n",
        "\n",
        "# remove the ones like 'artist: '\n",
        "original_csv = '/content/drive/MyDrive/data/dataset2/metadata.csv' # your path to the csv\n",
        "new_csv = '/content/drive/MyDrive/data/dataset2/metadata2.0.csv' # your path to new csv\n",
        "append_rows_to_csv(new_csv, ['file_name','lyrics'])\n",
        "with open(original_csv) as file:\n",
        "    content = file.readlines()\n",
        "\n",
        "dir = \"/content/drive/MyDrive/data/dataset2/train\"\n",
        "for filename in os.listdir(dir):\n",
        "    index = filename.split('.')[0].split('_')[1]\n",
        "    if content[int(index)+1].split(',',1)[1].__contains__('：'):\n",
        "      #delete meaningless chunk file\n",
        "      os.remove(os.path.join(dir,filename))\n",
        "      print(f\"{filename} deleted\")\n",
        "    else:\n",
        "      #write the meaningful row into new csv\n",
        "      # print([content[int(index)+1].split(',')[0],content[int(index)+1].split(',')[1]])\n",
        "      row_to_write = content[int(index)+1].strip().split(',')\n",
        "\n",
        "      # Ensure the newline character is removed from the end of the second element\n",
        "      row_to_write = [row_to_write[0], row_to_write[1].replace('\\n', '')]\n",
        "\n",
        "      # Write the split elements into the CSV file\n",
        "      append_rows_to_csv(new_csv, [row_to_write])\n",
        "\n",
        "dir = \"/content/drive/MyDrive/data/dataset2/test\"\n",
        "for filename in os.listdir(dir):\n",
        "    index = filename.split('.')[0].split('_')[1]\n",
        "    if content[int(index)+1].split(',',1)[1].__contains__('：'):\n",
        "      #delete meaningless chunk file\n",
        "      os.remove(os.path.join(dir,filename))\n",
        "      print(f\"{filename} deleted\")\n",
        "    else:\n",
        "      #write the meaningful row into new csv\n",
        "      # print([content[int(index)+1].split(',')[0],content[int(index)+1].split(',')[1]])\n",
        "      row_to_write = content[int(index)+1].strip().split(',')\n",
        "\n",
        "      # Ensure the newline character is removed from the end of the second element\n",
        "      row_to_write = [row_to_write[0], row_to_write[1].replace('\\n', '')]\n",
        "\n",
        "      # Write the split elements into the CSV file\n",
        "      append_rows_to_csv(new_csv, [row_to_write])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "with open('/content/metadata7.csv') as file:\n",
        "    content1 = file.readlines()"
      ],
      "metadata": {
        "id": "t1STdHQK5O1P",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_csv = \"/content/drive/MyDrive/data/dataset_cleaned/metadata.csv\""
      ],
      "metadata": {
        "id": "nZMMCP-Op4Fz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_and_clean_words(sentence):\n",
        "    # Find words containing an asterisk\n",
        "    words = sentence.split()\n",
        "    # Filter words that contain an asterisk\n",
        "    words_with_asterisk = [word for word in words if '*' in word]\n",
        "    # Clean each word to remove non-alphabetic and non-asterisk characters\n",
        "    cleaned_words = [''.join(re.findall(r'[A-Za-z*\\'-]', word)) for word in words_with_asterisk]\n",
        "    return cleaned_words\n",
        "\n",
        "# Example usage:\n",
        "sentence = \"Here's a sentence n***a with \"\"some*words, including*2asterisks, and others without.\"\n",
        "\n",
        "cleaned_words = extract_and_clean_words(sentence)\n",
        "\n",
        "print(cleaned_words)\n",
        "# Output: ['some*words', 'including*asterisks']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WgTp2d4uV30",
        "outputId": "7c64b284-01e9-4dd8-c02a-c3986ccb2d18"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['n***a', 'some*words', 'including*asterisks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title unmask\n",
        "# Opening JSON file\n",
        "f = open('unmask.json')\n",
        "dirty_map = json.load(f)\n",
        "\n",
        "with open(dir_csv) as file:\n",
        "    content = file.readlines()\n",
        "\n",
        "for i in content:\n",
        "    index = content.index(i)\n",
        "    lyric = i.split(',')[1]\n",
        "    if lyric.__contains__('*'):\n",
        "      dirty_words = extract_and_clean_words(lyric)\n",
        "      for j in dirty_words:\n",
        "        if j not in dirty_map.keys():\n",
        "          print(lyric)\n",
        "          print(f'error locating {j} in the map')\n",
        "        else:\n",
        "          content[index] = content[index].replace(j,dirty_map[j])\n",
        "          print(f'{j} replaced by {dirty_map[j]}')\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "id": "zNwK4e4kp-VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in content:\n",
        "      row_to_write = i.strip().split(',')\n",
        "      # Ensure the newline character is removed from the end of the second element\n",
        "      row_to_write = [row_to_write[0], row_to_write[1].replace('\\n', '')]\n",
        "      # print(row_to_write)\n",
        "      # Write the split elements into the CSV file\n",
        "      append_rows_to_csv('metadata8.0.csv', [row_to_write])"
      ],
      "metadata": {
        "id": "8LekZgTMDv7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title check number of chunks in total\n",
        "import os\n",
        "n = 0\n",
        "for i in os.listdir('/content/drive/MyDrive/data/dataset_cleaned/train'):\n",
        "  n = n+1\n",
        "training = n\n",
        "print(f\"number of training data: {n}\")\n",
        "for i in os.listdir('/content/drive/MyDrive/data/dataset_cleaned/test'):\n",
        "  n = n+1\n",
        "print(f\"number of testing data: {n-training}\")\n",
        "print(f\"number of data: {n}\")\n",
        "n_chunks = n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSTh4W1aMfxY",
        "outputId": "297c0e88-34bd-4f42-8b80-a86891cb955c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training data: 12553\n",
            "number of testing data: 3154\n",
            "number of data: 15707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7izzzNfWEQpd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}